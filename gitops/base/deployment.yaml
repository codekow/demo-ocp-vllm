---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  labels:
    deployment: vllm
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  replicas: 1
  selector:
    matchLabels:
      deployment: vllm
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        deployment: vllm
    spec:
      restartPolicy: Always
      containers:
        - name: vllm-server
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              value: ''
            - name: DOWNLOAD_MODEL
              value: ''
          command:
          - /bin/bash
          - -c |
              python3 -m vllm.entrypoints.openai.api_server \
              --model \
              "${DOWNLOAD_MODEL:-TinyLlama/TinyLlama-1.1B-Chat-v1.0}" \
              --download-dir /models-cache \
              --max-model-len  "2048"
          image: 'quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9:0.2'
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          resources:
            limits:
              cpu: '2'
              memory: 8Gi
            requests:
              cpu: '1'
              memory: 4Gi
          startupProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            timeoutSeconds: 5
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            timeoutSeconds: 8
            periodSeconds: 100
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: models-cache
              mountPath: /models-cache
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: vllm-cache
          persistentVolumeClaim:
            claimName: vllm-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
